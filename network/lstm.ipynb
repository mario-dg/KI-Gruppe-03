{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from logging import ERROR\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "import tensorflow as tf\n",
    "tf.get_logger().setLevel(ERROR)\n",
    "\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "import keras.backend as K\n",
    "from matplotlib.ticker import MultipleLocator,FormatStrFormatter \n",
    "\n",
    "from ipywidgets import IntSlider, ToggleButton, Checkbox, interact, HTML\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# df_weekly = pd.read_csv('../data/clean/df_weekly.csv')\n",
    "df_weekly = pd.read_csv('../data/clean/df_weekly_incidence.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Teile die In-Seqeuenz (Feature) in einzelne Sequenzen der Länge n_steps_in und\n",
    "die Out-Sequenz (Target) in einzelne Sequenzen der Länge n_steps_out.\n",
    "\n",
    "Beispiel: n_steps_in=1, n_steps_out=2\n",
    "           Feature  | Target\n",
    "    Seq 1:    1         -\n",
    "        |     2         -\n",
    "        |     3         6\n",
    "        |     -         8       \n",
    "\n",
    "@param overap \n",
    "Sollen sich die einzelnen Sequenzen überlappen?\n",
    "    Standardmäßig überlappen sich die Daten der Sequenzen:\n",
    "        1. Seq [1,2,3]\n",
    "        2. Seq [2,3,4]\n",
    "    mit overlap = False:\n",
    "        1. Seq [1,2,3]\n",
    "        2. Seq [4,5,6]\n",
    "\n",
    "'''\n",
    "def split_sequence(in_sequence, out_sequence, n_steps_in, n_steps_out, overlap=True, last_feature_as_target=True):\n",
    "    X, y = list(), list()\n",
    "    step = 1 if overlap else n_steps_in\n",
    "    left_over = 0 \n",
    "    target_offset = 1 if last_feature_as_target else 0\n",
    "\n",
    "    for i in range(0,len(in_sequence),step):\n",
    "        # find the end of this pattern\n",
    "        end_ix = i + n_steps_in\n",
    "        out_end_ix = end_ix + n_steps_out - target_offset\n",
    "        # check if we are beyond the sequence\n",
    "        if (out_end_ix > len(in_sequence)):\n",
    "            left_over = len(in_sequence) - i\n",
    "            break\n",
    "   \n",
    "        # gather input and output parts of the pattern     \n",
    "        seq_x, seq_y = in_sequence[i:end_ix,:],  out_sequence[end_ix-target_offset:out_end_ix,-1] #out_sequence[end_ix-1:out_end_ix,-1]\n",
    "        X.append(seq_x)\n",
    "        y.append(seq_y)\n",
    "\n",
    "       \n",
    "    return np.array(X), np.array(y), left_over"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Prüfe, ob in einer Sequenzen basieren auf den IN und OUT Steps \n",
    "LK übergreifenden Daten vorkommen.\n",
    "Entferne Zeilen, welche nicht mehr für eine volle Sequenz verwendet werden können\n",
    "\n",
    "BSP:\n",
    "    1. LK Erfurt\n",
    "    2. LK Erfurt\n",
    "    3. LK Erfurt\n",
    "    4. LK Ahrweiler\n",
    "    5. LK Ahrweiler\n",
    "\n",
    "'''\n",
    "def prepare_df_for_seqeuncing(df,n_steps_in, n_steps_out):\n",
    "    droped_indices = []\n",
    "    df_len = len(df)-1\n",
    "    for i in range(0,df_len):\n",
    "        # find the end of this pattern\n",
    "        end_ix = i + n_steps_in\n",
    "        out_end_ix = end_ix + n_steps_out -1\n",
    "        # check if we are beyond the sequence\n",
    "        if (out_end_ix > df_len):\n",
    "            break\n",
    "        start_row = df.iloc[i]\n",
    "        end_row = df.iloc[out_end_ix]\n",
    "\n",
    "        lk_of_seq_start = start_row.administrative_area_level_3\n",
    "        lk_of_seq_end = end_row.administrative_area_level_3\n",
    "\n",
    "        # Wenn es nicht mehr genug \"Zeilen\" eines LKs gibt müssen diese entfernt werden\n",
    "        if (lk_of_seq_start != lk_of_seq_end):\n",
    "            droped_indices.append(i)          \n",
    "    \n",
    "    df = df.drop(df.index[droped_indices])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DF Full: 58920\n"
     ]
    }
   ],
   "source": [
    "\n",
    "TIME_STEPS_IN = 30\n",
    "TIME_STEPS_OUT = 6\n",
    "DATA_OVERLAP = True\n",
    "# TARGET_COLUMN = \"incidence\"\n",
    "TARGET_COLUMN = \"\"\n",
    "TARGET_COLUMN = \"incidence\"\n",
    "TRAIN_DATA_COLUMNS = None\n",
    "TARGET_COLUMN_INDEX = 0\n",
    "\n",
    "print(\"DF Full:\",len(df_weekly))\n",
    "\n",
    "df_weekly_adjusted = prepare_df_for_seqeuncing(df_weekly,TIME_STEPS_IN, TIME_STEPS_OUT)\n",
    "df_weekly_adjusted.update(df_weekly_adjusted.select_dtypes(include=[np.number]).abs()) ## take absolute of every value\n",
    "data = df_weekly_adjusted[['confirmed', 'deaths' ,\"recovered\"\n",
    "    ,\"vaccines\",\"people_vaccinated\",\"people_fully_vaccinated\"\n",
    "    ,\"school_closing\",\"workplace_closing\",\"cancel_events\"\n",
    "    ,\"gatherings_restrictions\",\"transport_closing\"\n",
    "    ,\"stay_home_restrictions\",\"internal_movement_restrictions\"\n",
    "    ,\"internal_movement_restrictions\",\"information_campaigns\",\"testing_policy\"\n",
    "    ,\"contact_tracing\",\"facial_coverings\"\n",
    "    ,\"vaccination_policy\",\"elderly_people_protection\",\"population\",\"cfr\",\"cases_per_population\",\"incidence\"]]\n",
    "\n",
    "TRAIN_DATA_COLUMNS = data.columns\n",
    "TARGET_COLUMN_INDEX = TRAIN_DATA_COLUMNS.get_loc(TARGET_COLUMN)\n",
    "DATA_LK_NAMES = df_weekly_adjusted.administrative_area_level_3.unique()\n",
    "DATA_LK_COUNT = len(DATA_LK_NAMES)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data: (44535, 24)\n",
      "Features: (44535, 3) Target: (44535,)\n",
      "Features Scaled: (44535, 3) Target Scaled: (44535, 1)\n",
      "Features Seq.: (44501, 30, 3) Target Seq.: (44501, 6) DF Adjust.: 44501\n",
      "Train Size: 35600, Test Size: 8901\n",
      "----------------------------------------\n",
      "TRAIN: (35600, 30, 3) TEST: (8901, 30, 3)\n",
      "[[1.45751638e-05 0.00000000e+00]\n",
      " [6.24649877e-05 0.00000000e+00]\n",
      " [1.13856637e-04 2.68283522e-04]\n",
      " [1.61841104e-04 3.75596931e-04]\n",
      " [2.09021099e-04 3.75596931e-04]\n",
      " [2.24590024e-04 3.75596931e-04]\n",
      " [2.30552591e-04 3.75596931e-04]\n",
      " [2.51222823e-04 3.75596931e-04]\n",
      " [2.83818189e-04 3.75596931e-04]\n",
      " [3.18666081e-04 3.75596931e-04]\n",
      " [3.58889747e-04 3.75596931e-04]\n",
      " [3.72329184e-04 3.75596931e-04]\n",
      " [3.82266796e-04 3.75596931e-04]\n",
      " [3.88560616e-04 3.75596931e-04]\n",
      " [3.91541900e-04 3.75596931e-04]\n",
      " [4.08932720e-04 3.75596931e-04]\n",
      " [4.25329780e-04 3.75596931e-04]\n",
      " [4.33942376e-04 5.00795908e-04]\n",
      " [4.43217481e-04 7.51193862e-04]\n",
      " [4.48186286e-04 7.51193862e-04]\n",
      " [4.66499885e-04 7.51193862e-04]\n",
      " [4.91249270e-04 7.51193862e-04]\n",
      " [5.30005956e-04 1.00159182e-03]\n",
      " [5.68715320e-04 1.12679079e-03]\n",
      " [6.22425745e-04 1.12679079e-03]\n",
      " [6.61182430e-04 1.12679079e-03]\n",
      " [6.76088848e-04 1.12679079e-03]\n",
      " [6.97123459e-04 1.12679079e-03]\n",
      " [7.21139354e-04 1.12679079e-03]\n",
      " [7.68603280e-04 1.12679079e-03]] [0.00487394 0.00498534 0.01026776 0.02506596 0.03520375 0.03771035]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "data = np.array(data)\n",
    "\n",
    "print(\"Data:\",data.shape)\n",
    "\n",
    "# Vorbereitung der Eingangsdaten (Features) und der Zielvariablen\n",
    "# features = data[:, :] \n",
    "# target = data[:, -1]\n",
    "\n",
    "\n",
    "target = data[:, -1]\n",
    "features = data[:,:3] ## nur die ersten 3 columns\n",
    "print(\"Features:\",features.shape,\"Target:\",target.shape)\n",
    "\n",
    "N_FEATURES = features.shape[-1]\n",
    "\n",
    "feat_range = (-1,1) # tanh\n",
    "# feat_range = (0,1) # relu\n",
    "\n",
    "# Daten normalisieren\n",
    "feature_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "target_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "features_scaled = feature_scaler.fit_transform(features)\n",
    "target_scaled = target_scaler.fit_transform(target.reshape(-1, 1))\n",
    "print(\"Features Scaled:\",features_scaled.shape,\"Target Scaled:\",target_scaled.shape)\n",
    "## SCALING Muss nach sequenzierung passieren, sonst passt die Normaliseirung nicht, wenn Werte fehlen\n",
    "\n",
    "feature_seqs, target_seq, left_over =  split_sequence(features_scaled,target_scaled, TIME_STEPS_IN, TIME_STEPS_OUT, overlap=DATA_OVERLAP)\n",
    "df_weekly_adjusted = df_weekly_adjusted.drop(df_weekly_adjusted.index[-left_over:])\n",
    "print(\"Features Seq.:\",feature_seqs.shape,\"Target Seq.:\",target_seq.shape, \"DF Adjust.:\",len(df_weekly_adjusted))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "train_size = int(len(feature_seqs) * 0.8)\n",
    "test_size = len(feature_seqs) - train_size\n",
    "print(\"Train Size: {}, Test Size: {}\".format(train_size,test_size))\n",
    "\n",
    "\n",
    "train_features_lstm, test_features_lstm = feature_seqs[:train_size], feature_seqs[train_size:]\n",
    "train_target_lstm, test_target_lstm = target_seq[:train_size], target_seq[train_size:]\n",
    "\n",
    "print(\"----------------------------------------\")\n",
    "print(\"TRAIN:\",train_features_lstm.shape,\"TEST:\",test_features_lstm.shape)\n",
    "print(train_features_lstm[0,:,:2],train_target_lstm[0,:])\n",
    "# print(train_features_lstm[1,:,:2],train_target_lstm[1,:])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (1, 30, 300)              364800    \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (1, 300)                  721200    \n",
      "                                                                 \n",
      " repeat_vector (RepeatVector  (1, 6, 300)              0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " lstm_2 (LSTM)               (1, 6, 300)               721200    \n",
      "                                                                 \n",
      " lstm_3 (LSTM)               (1, 6, 300)               721200    \n",
      "                                                                 \n",
      " time_distributed (TimeDistr  (1, 6, 1)                301       \n",
      " ibuted)                                                         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,528,701\n",
      "Trainable params: 2,528,701\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "''' \n",
    "Encoder-Decoder Architecture für Sequence to Sequence\n",
    "https://machinelearningmastery.com/encoder-decoder-long-short-term-memory-networks/\n",
    "\n",
    "'''\n",
    "K.clear_session()\n",
    "\n",
    "stateful = False\n",
    "activ = 'tanh'\n",
    "activ_dec = 'elu'\n",
    "# activ = 'relu'\n",
    "# loss = tf.keras.losses.MeanSquaredError()\n",
    "loss = \"mse\"\n",
    "# loss = tf.keras.losses.CategoricalCrossentropy()\n",
    "optim = tf.keras.optimizers.Adam()\n",
    "\n",
    "ndim = 300\n",
    "ndim2 = 100\n",
    "\n",
    "model = Sequential([\n",
    "    LSTM(ndim, activation=activ, batch_input_shape=(1, TIME_STEPS_IN, N_FEATURES),  input_shape=(TIME_STEPS_IN, N_FEATURES), return_sequences=True,stateful=stateful),\n",
    "    LSTM(ndim, activation=activ,stateful=stateful),\n",
    "    tf.keras.layers.RepeatVector(TIME_STEPS_OUT), # damit der Output 3d ([samples, time steps, features]) \n",
    "    LSTM(ndim, activation=activ, return_sequences=True,stateful=stateful), # return_sequences: Gibt den gesamten Output Eingabesequenz zurück und nicht nur den letzen\n",
    "    LSTM(ndim, activation=activ, return_sequences=True,stateful=stateful), # return_sequences: Gibt den gesamten Output Eingabesequenz zurück und nicht nur den letzen\n",
    "    \n",
    "    # tf.keras.layers.TimeDistributed(Dense(ndim2, activation=activ)),\n",
    "    \n",
    "    tf.keras.layers.TimeDistributed(Dense(1)),\n",
    "])\n",
    "\n",
    "model.compile(optimizer=optim, loss=loss,metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # returns train, inference_encoder and inference_decoder models\n",
    "# def define_models(n_input, n_output, n_units):\n",
    "#     # define training encoder\n",
    "#     # encoder_inputs = tf.keras.layers.Input(shape=(None, n_input))\n",
    "#     encoder_inputs = tf.keras.layers.Input(shape=(None, n_input))\n",
    "#     encoder = LSTM(n_units, return_state=True)\n",
    "#     encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "#     encoder_states = [state_h, state_c]\n",
    "#     # define training decoder\n",
    "#     decoder_inputs = tf.keras.layers.Input(shape=(None, n_output))\n",
    "#     decoder_lstm = LSTM(n_units, return_sequences=True, return_state=True)\n",
    "#     decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
    "#     decoder_dense = Dense(n_output, activation='softmax')\n",
    "#     decoder_outputs = decoder_dense(decoder_outputs)\n",
    "#     model = tf.keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "#     # define inference encoder\n",
    "#     encoder_model = tf.keras.Model(encoder_inputs, encoder_states)\n",
    "#     # define inference decoder\n",
    "#     decoder_state_input_h = tf.keras.layers.Input(shape=(n_units,))\n",
    "#     decoder_state_input_c = tf.keras.layers.Input(shape=(n_units,))\n",
    "#     decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "#     decoder_outputs, state_h, state_c = decoder_lstm(decoder_inputs, initial_state=decoder_states_inputs)\n",
    "#     decoder_states = [state_h, state_c]\n",
    "#     decoder_outputs = decoder_dense(decoder_outputs)\n",
    "#     decoder_model = tf.keras.Model([decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states)\n",
    "#     # return all models\n",
    "#     return model, encoder_model, decoder_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model, inference_encoder, inference_decoder = define_models(TIME_STEPS_IN,TIME_STEPS_OUT,20)\n",
    "# model.compile(optimizer='adam', loss='mse',metrics=['accuracy'])\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lk_data(step):\n",
    "    lk_data = df_weekly_adjusted[df_weekly_adjusted.administrative_area_level_3 == DATA_LK_NAMES[step%int(train_size/DATA_LK_COUNT)]]\n",
    "    lk_data_start = lk_data.index.values[0]\n",
    "    lk_len = len(lk_data)\n",
    "    return (train_features_lstm[lk_data_start:lk_data_start+lk_len,:,:], train_target_lstm[lk_data_start:lk_data_start+lk_len,:]),\\\n",
    "           (test_features_lstm[lk_data_start:lk_data_start+lk_len,:,:], test_target_lstm[lk_data_start:lk_data_start+lk_len,:]), DATA_LK_NAMES[step%DATA_LK_COUNT]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "557/557 [==============================] - 93s 166ms/step - loss: 0.0120 - accuracy: 0.0113 - val_loss: 0.0116 - val_accuracy: 0.0180\n",
      "Epoch 2/25\n",
      "557/557 [==============================] - 94s 169ms/step - loss: 0.0097 - accuracy: 0.0113 - val_loss: 0.0106 - val_accuracy: 0.0180\n",
      "Epoch 3/25\n",
      "269/557 [=============>................] - ETA: 44s - loss: 0.0077 - accuracy: 0.0106"
     ]
    }
   ],
   "source": [
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "# REPS = 5\n",
    "TRAIN_LKS = int(train_size/DATA_LK_COUNT) \n",
    "EPOCHS = 25\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "CUSTOM_TRAIN = False\n",
    "\n",
    "loss = None\n",
    "history = None\n",
    "\n",
    "if CUSTOM_TRAIN:\n",
    "\n",
    "    '''\n",
    "    Custom Training Loop \n",
    "    - für Stateful LSTMs \n",
    "    - für ein LK pro Epoch\n",
    "    '''\n",
    "\n",
    "    print(f\"Training for {EPOCHS} epochs!\")\n",
    "    for i in range(EPOCHS):    \n",
    "        for j in range(TRAIN_LKS):\n",
    "            (train,test,lk) = get_lk_data(j) \n",
    "            # print(f'EPOCH: {i: <4} - {lk: <37}',end=' | ',flush=True)\n",
    "            train_x, train_y = train\n",
    "            model.fit(train_x, train_y , epochs=1, batch_size=BATCH_SIZE, verbose=0, shuffle=False,validation_data=(test),)\n",
    "        # model.reset_states()\n",
    "        print(f'EPOCH: {i: <4}',end=' | ',flush=True)\n",
    "        epoch_val_loss = model.evaluate(test_features_lstm, test_target_lstm,batch_size=BATCH_SIZE)\n",
    "    loss = model.evaluate(test_features_lstm, test_target_lstm,batch_size=1)\n",
    "\n",
    "else:\n",
    "\n",
    "    class haltCallback(tf.keras.callbacks.Callback):\n",
    "        def on_epoch_end(self, epoch, logs={}):\n",
    "            stop_at = 0.03\n",
    "            if(logs.get('loss') <= stop_at):\n",
    "                print(f\"\\n\\n\\nReached {stop_at} loss value so cancelling training!\\n\\n\\n\")\n",
    "                self.model.stop_training = True\n",
    "\n",
    "\n",
    "    '''\n",
    "    Normaler Training Loop \n",
    "    '''\n",
    "## Trainiere                                                \n",
    "    history = model.fit(\n",
    "                        train_features_lstm,\n",
    "                        # [train_features_lstm,train_features_lstm_decoder],\n",
    "                        train_target_lstm, \n",
    "                        epochs=EPOCHS, \n",
    "                        batch_size=BATCH_SIZE, \n",
    "                        shuffle=False,\n",
    "                        validation_data=(\n",
    "                            test_features_lstm,\n",
    "                            # [test_features_lstm,test_features_lstm_decoder],\n",
    "                            test_target_lstm),\n",
    "                        callbacks=[\n",
    "                            tensorboard_callback,\n",
    "                            # haltCallback()\n",
    "                            tf.keras.callbacks.EarlyStopping(monitor='loss', patience=200),\n",
    "                            ],\n",
    "                        )\n",
    "\n",
    "    loss = model.evaluate(test_features_lstm, test_target_lstm,batch_size=BATCH_SIZE)\n",
    "\n",
    "\n",
    "print('Test Loss:', loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save(\"model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# make a one-step forecast\n",
    "def forecast_lstm(model, batch_size, X):\n",
    "    X = X.reshape(1, X.shape[0],X.shape[1])\n",
    "    yhat = model.predict(X, batch_size=batch_size,verbose=0)\n",
    "    yhat_2 = target_scaler.inverse_transform(yhat.reshape(-1,1)).reshape(-1)\n",
    "    return yhat_2[0]\n",
    "\n",
    "''' \n",
    "Walk forward validation.\n",
    "\n",
    "Vorhersage für einzelne Zeitschritte über ein vorgegeben Zeitraum, mit sich verändernden Input-Daten.\n",
    "Vorhersage für Zeitschritt t mit den Input-Daten von t bis t-TIME_STEPS_IN. \n",
    "Der Zeitschritt für welchen vorher eine Vorhersage gemacht werden sollte, ist jetzt teil der Input-Daten\n",
    "\n",
    "'''\n",
    "def walk_forward_validation(lstm_model, test_features, test_target,pred_steps, start_point=0):\n",
    "    expect_predict = list()\n",
    "    for i in range(start_point,start_point+pred_steps,1):\n",
    "        # make one-step forecast\n",
    "        X, y = test_features[i,:,:], test_target[i,:] # get the whole sequence\n",
    "        \n",
    "        yhat = forecast_lstm(lstm_model, 1, X)\n",
    "        expec = target_scaler.inverse_transform(y[0].reshape(-1,1)).reshape(-1)[0]\n",
    "        expect_predict.append(np.array([expec,yhat]))\n",
    "        # print('Week=%d, Predicted=%f, Expected=%f' % (i+1, yhat, expec))\n",
    "    return np.array(expect_predict).reshape(pred_steps,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13e40576f8be43fa80625e2cda85016d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, continuous_update=False, description='INDEX', max=8901), Checkbox(val…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@interact(INDEX=IntSlider(min=0,max=len(test_features_lstm),step=1,value=0,continuous_update=False),show_input_data=Checkbox(value=False))\n",
    "def show_validation(INDEX,show_input_data):\n",
    "    # INDEX = 200\n",
    "    TEST_INDEX_L = INDEX \n",
    "    TEST_INDEX_H = INDEX+1\n",
    "    val_features = test_features_lstm[TEST_INDEX_L:TEST_INDEX_H]\n",
    "    val_target = test_target_lstm[TEST_INDEX_L:TEST_INDEX_H]\n",
    "\n",
    "    ## * IN Sequence  | Time Steps die nur reingegeben werden\n",
    "    ## * OUT Sequence | Time Steps die vom Netzwerk predicted werden\n",
    "    ## * TIME_STEPS_OUT, da die Prediction für zwei Zeitschritte aus der Prediction des letzen Val-Wertes und des ersten komplett unbekannten besteht.\n",
    "    ## -> ÜBerscheiden sich am Ende \n",
    "    start_test_data = train_size\n",
    "    start_val_data = start_test_data +TEST_INDEX_L\n",
    "    end_val_data = (TEST_INDEX_H-TEST_INDEX_L)*TIME_STEPS_IN\n",
    "    end_prediciton_vals= TIME_STEPS_OUT -1\n",
    "\n",
    "    df_weekly_val = df_weekly_adjusted.iloc[start_val_data:start_val_data+end_val_data]\n",
    "\n",
    "    start_lk = HTML(\"<b>START LK</b>: {}\".format(df_weekly_val.iloc[0].administrative_area_level_3))\n",
    "    end_lk = HTML(\"<b>END LK</b>: {}\".format(df_weekly_val.iloc[-1].administrative_area_level_3))\n",
    "    display(start_lk)\n",
    "    display(end_lk)\n",
    "\n",
    "\n",
    "    ## zur Kontrolle: de-scalierte Features\n",
    "    de_time_series_val_fatures = val_features.reshape((TEST_INDEX_H-TEST_INDEX_L)*TIME_STEPS_IN,N_FEATURES)\n",
    "    de_scaled_val_features = feature_scaler.inverse_transform(de_time_series_val_fatures)\n",
    "    de_scaled_target_features = target_scaler.inverse_transform(val_target.reshape(-1,1)).reshape(-1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    expect_target_rows = df_weekly_adjusted.iloc[start_val_data+end_val_data-1:start_val_data+end_val_data+end_prediciton_vals]\n",
    "\n",
    "\n",
    "    prediction = model.predict(val_features)\n",
    "    de_scaled_prediction = target_scaler.inverse_transform(prediction.reshape(-1,1)).reshape(-1)\n",
    "    # prediction\n",
    "\n",
    "    print(f\"Sequence Prediction for: {TARGET_COLUMN}\")\n",
    "    values_is = expect_target_rows[TARGET_COLUMN].to_numpy()\n",
    "\n",
    "    result_df = pd.DataFrame(data={\n",
    "        \"Row\":expect_target_rows.index.values,\n",
    "        \"Week\":expect_target_rows.week.values,\n",
    "        \"Predicted\":de_scaled_prediction,\n",
    "        \"Expected\":values_is,\n",
    "        \"abs(Diff)\":abs(de_scaled_prediction -  values_is)})\n",
    "\n",
    "    display(result_df)\n",
    "\n",
    "    print(\"Val Expected Target\")\n",
    "    display(expect_target_rows.iloc[:,[0,1,2,3,4,5,-1]])\n",
    "\n",
    "\n",
    "    if show_input_data:\n",
    "\n",
    "        print(\"Kontrolle: de-skalierte Features:\")\n",
    "        print(\" * Zeile {} | confirmed: {:.2f}, recovered: {:.2f}\".format(df_weekly_val.index.values[0],de_scaled_val_features[0,0],de_scaled_val_features[0,1]))\n",
    "        print(\"\\nVal Input Features:\")\n",
    "        display(df_weekly_val)\n",
    "\n",
    "\n",
    "    walk_distance = TIME_STEPS_OUT\n",
    "    pred_data = walk_forward_validation(model,test_features_lstm,test_target_lstm,walk_distance,INDEX)\n",
    "    x_axis = np.arange(0,pred_data.shape[0],1,dtype=np.int32)\n",
    "    x_axis = expect_target_rows.week.values\n",
    "    fig, axs = plt.subplots(figsize=(25,6))\n",
    "    axs.plot(x_axis,pred_data[:,0])\n",
    "    legend = [\"Expected\",\"Predicted (Seq.)\",\"Predicted (Walk)\"]\n",
    "    axs.plot(x_axis,result_df[\"Predicted\"].to_numpy())\n",
    "    axs.plot(x_axis,pred_data[:,1],'--')\n",
    "    axs.legend(legend)\n",
    "    # axs.xaxis.set_major_locator(MultipleLocator(1)) \n",
    "    axs.xaxis.set_major_formatter(FormatStrFormatter('%d'))\n",
    "    axs.set_xlabel(\"Weeks\")\n",
    "    axs.set_ylabel(\"Incidence\")\n",
    "    axs.grid(\"on\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "981ab37503f944cdab5f06bab1695c8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, continuous_update=False, description='INDEX', max=8901), IntSlider(va…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@interact(INDEX=IntSlider(min=0,max=len(test_features_lstm),step=1,value=0,continuous_update=False),walk_distance=IntSlider(min=0,max=30,step=1,value=20,continuous_update=False))\n",
    "def walk_foreward_val(INDEX,walk_distance):\n",
    "    # walk_distance = 20\n",
    "    # INDEX = 250\n",
    "    pred_data = walk_forward_validation(model,test_features_lstm,test_target_lstm,walk_distance,INDEX)\n",
    "    x_axis = np.arange(0,pred_data.shape[0],1,dtype=np.int32)\n",
    "\n",
    "    fig, axs = plt.subplots(figsize=(20,6))\n",
    "    axs.plot(x_axis,pred_data)\n",
    "    legend = [\"Expected\",\"Predicted (Walk)\"]\n",
    "    axs.legend(legend)\n",
    "    # axs.xaxis.set_major_locator(MultipleLocator(1)) \n",
    "    axs.xaxis.set_major_formatter(FormatStrFormatter('%d'))\n",
    "    axs.set_xlabel(\"Weeks\")\n",
    "    axs.set_ylabel(\"Incidence\")\n",
    "    axs.grid(\"on\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_test_data = train_size\n",
    "df_weekly_val = df_weekly_adjusted.loc[start_test_data:]\n",
    "\n",
    "\n",
    "large_diff_inputs = pd.DataFrame()\n",
    "df_len = len(df_weekly_val)\n",
    "for i in range(0,len(test_features_lstm),TIME_STEPS_IN):\n",
    "        if i+TIME_STEPS_IN >= test_size:\n",
    "                break\n",
    "        val_features = test_features_lstm[i,:,:].reshape(1,TIME_STEPS_IN,N_FEATURES)\n",
    "        val_target = test_target_lstm[i,:].reshape(1,TIME_STEPS_OUT)\n",
    "\n",
    "        prediction = model.predict(val_features,verbose=0)\n",
    "        de_scaled_target_features = target_scaler.inverse_transform(val_target.reshape(-1,1)).reshape(-1)\n",
    "        de_scaled_prediction = target_scaler.inverse_transform(prediction.reshape(-1,1)).reshape(-1)\n",
    "        abs_diff = abs(de_scaled_prediction-de_scaled_target_features)\n",
    "\n",
    "        preds = np.append(np.zeros(TIME_STEPS_IN-TIME_STEPS_OUT)-1,de_scaled_prediction)\n",
    "        for d in abs_diff:\n",
    "                if d >= 100.0:\n",
    "                        len_diff = len(large_diff_inputs)\n",
    "                        large_diff_inputs = pd.concat([large_diff_inputs,df_weekly_val.iloc[i:i+TIME_STEPS_IN]])\n",
    "                        try:    \n",
    "                                large_diff_inputs.loc[large_diff_inputs.index[len_diff:len_diff+TIME_STEPS_IN],\"Predicted\"] = preds\n",
    "                        except ValueError:\n",
    "                                print(\"ERROR: \",i,i+TIME_STEPS_IN,len(large_diff_inputs))\n",
    "                        break\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>administrative_area_level_3</th>\n",
       "      <th>year</th>\n",
       "      <th>week</th>\n",
       "      <th>confirmed</th>\n",
       "      <th>deaths</th>\n",
       "      <th>recovered</th>\n",
       "      <th>vaccines</th>\n",
       "      <th>people_vaccinated</th>\n",
       "      <th>people_fully_vaccinated</th>\n",
       "      <th>school_closing</th>\n",
       "      <th>...</th>\n",
       "      <th>facial_coverings</th>\n",
       "      <th>vaccination_policy</th>\n",
       "      <th>elderly_people_protection</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>population</th>\n",
       "      <th>cfr</th>\n",
       "      <th>cases_per_population</th>\n",
       "      <th>incidence</th>\n",
       "      <th>Predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>35600</th>\n",
       "      <td>LK Stormarn</td>\n",
       "      <td>2021</td>\n",
       "      <td>51</td>\n",
       "      <td>11961.500000</td>\n",
       "      <td>346.000000</td>\n",
       "      <td>11615.500000</td>\n",
       "      <td>485966.500000</td>\n",
       "      <td>198702.833333</td>\n",
       "      <td>195663.500000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>53.707825</td>\n",
       "      <td>10.305331</td>\n",
       "      <td>243196.0</td>\n",
       "      <td>0.028930</td>\n",
       "      <td>0.049185</td>\n",
       "      <td>214.504625</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35601</th>\n",
       "      <td>LK Stormarn</td>\n",
       "      <td>2021</td>\n",
       "      <td>52</td>\n",
       "      <td>12490.400000</td>\n",
       "      <td>348.400000</td>\n",
       "      <td>12142.000000</td>\n",
       "      <td>494748.400000</td>\n",
       "      <td>199418.600000</td>\n",
       "      <td>196242.400000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>53.707825</td>\n",
       "      <td>10.305331</td>\n",
       "      <td>243196.0</td>\n",
       "      <td>0.027899</td>\n",
       "      <td>0.051359</td>\n",
       "      <td>229.609040</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35602</th>\n",
       "      <td>LK Stormarn</td>\n",
       "      <td>2021</td>\n",
       "      <td>53</td>\n",
       "      <td>3259.500000</td>\n",
       "      <td>182.500000</td>\n",
       "      <td>3077.000000</td>\n",
       "      <td>1683.000000</td>\n",
       "      <td>1683.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>53.707825</td>\n",
       "      <td>10.305331</td>\n",
       "      <td>243196.0</td>\n",
       "      <td>0.055989</td>\n",
       "      <td>0.013403</td>\n",
       "      <td>133.637066</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35603</th>\n",
       "      <td>LK Stormarn</td>\n",
       "      <td>2022</td>\n",
       "      <td>1</td>\n",
       "      <td>13574.571429</td>\n",
       "      <td>350.857143</td>\n",
       "      <td>13223.714286</td>\n",
       "      <td>507396.000000</td>\n",
       "      <td>200202.428571</td>\n",
       "      <td>197494.142857</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>53.707825</td>\n",
       "      <td>10.305331</td>\n",
       "      <td>243196.0</td>\n",
       "      <td>0.025861</td>\n",
       "      <td>0.055817</td>\n",
       "      <td>408.253895</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35604</th>\n",
       "      <td>LK Stormarn</td>\n",
       "      <td>2022</td>\n",
       "      <td>2</td>\n",
       "      <td>14512.285714</td>\n",
       "      <td>352.142857</td>\n",
       "      <td>14160.142857</td>\n",
       "      <td>524800.000000</td>\n",
       "      <td>201148.571429</td>\n",
       "      <td>199551.571429</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>53.707825</td>\n",
       "      <td>10.305331</td>\n",
       "      <td>243196.0</td>\n",
       "      <td>0.024275</td>\n",
       "      <td>0.059673</td>\n",
       "      <td>385.579650</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47380</th>\n",
       "      <td>SK Emden</td>\n",
       "      <td>2021</td>\n",
       "      <td>21</td>\n",
       "      <td>1259.400000</td>\n",
       "      <td>13.600000</td>\n",
       "      <td>1245.800000</td>\n",
       "      <td>25072.600000</td>\n",
       "      <td>18482.200000</td>\n",
       "      <td>6590.400000</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>53.363747</td>\n",
       "      <td>7.196228</td>\n",
       "      <td>50195.0</td>\n",
       "      <td>0.010796</td>\n",
       "      <td>0.025090</td>\n",
       "      <td>121.924494</td>\n",
       "      <td>948.843018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47381</th>\n",
       "      <td>SK Emden</td>\n",
       "      <td>2021</td>\n",
       "      <td>22</td>\n",
       "      <td>1307.500000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>1293.500000</td>\n",
       "      <td>27286.500000</td>\n",
       "      <td>19412.666667</td>\n",
       "      <td>7873.833333</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>53.363747</td>\n",
       "      <td>7.196228</td>\n",
       "      <td>50195.0</td>\n",
       "      <td>0.010708</td>\n",
       "      <td>0.026048</td>\n",
       "      <td>105.256168</td>\n",
       "      <td>890.517395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47382</th>\n",
       "      <td>SK Emden</td>\n",
       "      <td>2021</td>\n",
       "      <td>23</td>\n",
       "      <td>1318.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>1304.000000</td>\n",
       "      <td>30256.666667</td>\n",
       "      <td>20433.666667</td>\n",
       "      <td>9823.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>53.363747</td>\n",
       "      <td>7.196228</td>\n",
       "      <td>50195.0</td>\n",
       "      <td>0.010622</td>\n",
       "      <td>0.026258</td>\n",
       "      <td>20.918418</td>\n",
       "      <td>687.630005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47383</th>\n",
       "      <td>SK Emden</td>\n",
       "      <td>2021</td>\n",
       "      <td>24</td>\n",
       "      <td>1323.333333</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>1309.333333</td>\n",
       "      <td>33691.333333</td>\n",
       "      <td>21406.500000</td>\n",
       "      <td>12284.833333</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>53.363747</td>\n",
       "      <td>7.196228</td>\n",
       "      <td>50195.0</td>\n",
       "      <td>0.010579</td>\n",
       "      <td>0.026364</td>\n",
       "      <td>10.625228</td>\n",
       "      <td>543.169373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47384</th>\n",
       "      <td>SK Emden</td>\n",
       "      <td>2021</td>\n",
       "      <td>25</td>\n",
       "      <td>1327.166667</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>1313.166667</td>\n",
       "      <td>37341.166667</td>\n",
       "      <td>22828.166667</td>\n",
       "      <td>14513.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>53.363747</td>\n",
       "      <td>7.196228</td>\n",
       "      <td>50195.0</td>\n",
       "      <td>0.010549</td>\n",
       "      <td>0.026440</td>\n",
       "      <td>7.636883</td>\n",
       "      <td>484.422180</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5430 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      administrative_area_level_3  year  week     confirmed      deaths  \\\n",
       "35600                 LK Stormarn  2021    51  11961.500000  346.000000   \n",
       "35601                 LK Stormarn  2021    52  12490.400000  348.400000   \n",
       "35602                 LK Stormarn  2021    53   3259.500000  182.500000   \n",
       "35603                 LK Stormarn  2022     1  13574.571429  350.857143   \n",
       "35604                 LK Stormarn  2022     2  14512.285714  352.142857   \n",
       "...                           ...   ...   ...           ...         ...   \n",
       "47380                    SK Emden  2021    21   1259.400000   13.600000   \n",
       "47381                    SK Emden  2021    22   1307.500000   14.000000   \n",
       "47382                    SK Emden  2021    23   1318.000000   14.000000   \n",
       "47383                    SK Emden  2021    24   1323.333333   14.000000   \n",
       "47384                    SK Emden  2021    25   1327.166667   14.000000   \n",
       "\n",
       "          recovered       vaccines  people_vaccinated  \\\n",
       "35600  11615.500000  485966.500000      198702.833333   \n",
       "35601  12142.000000  494748.400000      199418.600000   \n",
       "35602   3077.000000    1683.000000        1683.000000   \n",
       "35603  13223.714286  507396.000000      200202.428571   \n",
       "35604  14160.142857  524800.000000      201148.571429   \n",
       "...             ...            ...                ...   \n",
       "47380   1245.800000   25072.600000       18482.200000   \n",
       "47381   1293.500000   27286.500000       19412.666667   \n",
       "47382   1304.000000   30256.666667       20433.666667   \n",
       "47383   1309.333333   33691.333333       21406.500000   \n",
       "47384   1313.166667   37341.166667       22828.166667   \n",
       "\n",
       "       people_fully_vaccinated  school_closing  ...  facial_coverings  \\\n",
       "35600            195663.500000             1.0  ...               2.0   \n",
       "35601            196242.400000             1.0  ...               2.0   \n",
       "35602                 0.000000             3.0  ...               2.0   \n",
       "35603            197494.142857             1.0  ...               2.0   \n",
       "35604            199551.571429             1.0  ...               2.0   \n",
       "...                        ...             ...  ...               ...   \n",
       "47380              6590.400000             3.0  ...               2.0   \n",
       "47381              7873.833333             1.0  ...               2.0   \n",
       "47382              9823.000000             1.0  ...               2.0   \n",
       "47383             12284.833333             1.0  ...               2.0   \n",
       "47384             14513.000000             1.0  ...               2.0   \n",
       "\n",
       "       vaccination_policy  elderly_people_protection   latitude  longitude  \\\n",
       "35600                 5.0                        2.0  53.707825  10.305331   \n",
       "35601                 5.0                        2.0  53.707825  10.305331   \n",
       "35602                 2.0                        2.0  53.707825  10.305331   \n",
       "35603                 5.0                        2.0  53.707825  10.305331   \n",
       "35604                 5.0                        2.0  53.707825  10.305331   \n",
       "...                   ...                        ...        ...        ...   \n",
       "47380                 3.0                        2.0  53.363747   7.196228   \n",
       "47381                 3.0                        2.0  53.363747   7.196228   \n",
       "47382                 5.0                        2.0  53.363747   7.196228   \n",
       "47383                 5.0                        2.0  53.363747   7.196228   \n",
       "47384                 5.0                        2.0  53.363747   7.196228   \n",
       "\n",
       "       population       cfr  cases_per_population   incidence   Predicted  \n",
       "35600    243196.0  0.028930              0.049185  214.504625   -1.000000  \n",
       "35601    243196.0  0.027899              0.051359  229.609040   -1.000000  \n",
       "35602    243196.0  0.055989              0.013403  133.637066   -1.000000  \n",
       "35603    243196.0  0.025861              0.055817  408.253895   -1.000000  \n",
       "35604    243196.0  0.024275              0.059673  385.579650   -1.000000  \n",
       "...           ...       ...                   ...         ...         ...  \n",
       "47380     50195.0  0.010796              0.025090  121.924494  948.843018  \n",
       "47381     50195.0  0.010708              0.026048  105.256168  890.517395  \n",
       "47382     50195.0  0.010622              0.026258   20.918418  687.630005  \n",
       "47383     50195.0  0.010579              0.026364   10.625228  543.169373  \n",
       "47384     50195.0  0.010549              0.026440    7.636883  484.422180  \n",
       "\n",
       "[5430 rows x 30 columns]"
      ]
     },
     "execution_count": 332,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "large_diff_inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction for: \u001b[33m\u001b[2mincidence\u001b[0m\n",
      "##################################################\n",
      "    \u001b[34m\u001b[1mRow\u001b[0m    | \u001b[32m\u001b[1mExpected\u001b[0m  | \u001b[35m\u001b[1mPredicted\u001b[0m |  \u001b[31m\u001b[1mabs(Diff)\u001b[0m\n",
      "--------------------------------------------------\n",
      "   \u001b[34m\u001b[2m47390\u001b[0m   |   \u001b[32m\u001b[2m32.4\u001b[0m    |   \u001b[35m\u001b[2m33.9\u001b[0m    |    \u001b[31m\u001b[2m1.5\u001b[0m   \n",
      "   \u001b[34m\u001b[2m47391\u001b[0m   |   \u001b[32m\u001b[2m22.0\u001b[0m    |   \u001b[35m\u001b[2m14.3\u001b[0m    |    \u001b[31m\u001b[2m7.7\u001b[0m   \n",
      "   \u001b[34m\u001b[2m47392\u001b[0m   |   \u001b[32m\u001b[2m33.7\u001b[0m    |   \u001b[35m\u001b[2m27.9\u001b[0m    |    \u001b[31m\u001b[2m5.8\u001b[0m   \n",
      "   \u001b[34m\u001b[2m47393\u001b[0m   |   \u001b[32m\u001b[2m47.5\u001b[0m    |   \u001b[35m\u001b[2m59.8\u001b[0m    |   \u001b[31m\u001b[2m12.2\u001b[0m   \n",
      "   \u001b[34m\u001b[2m47394\u001b[0m   |   \u001b[32m\u001b[2m61.7\u001b[0m    |   \u001b[35m\u001b[2m79.7\u001b[0m    |   \u001b[31m\u001b[2m18.0\u001b[0m   \n",
      "   \u001b[34m\u001b[2m47395\u001b[0m   |   \u001b[32m\u001b[2m66.7\u001b[0m    |   \u001b[35m\u001b[2m68.1\u001b[0m    |    \u001b[31m\u001b[2m1.4\u001b[0m   \n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "# def coloring(string,color=\"normal\",style=\"normal\"):\n",
    "#     col = \"0\"\n",
    "#     sty = \"2\"\n",
    "#     if color == \"blink\":\n",
    "#         col = \"5\"\n",
    "#     elif   color == \"red\":\n",
    "#         col = \"31\"\n",
    "#     elif color == \"green\":\n",
    "#         col = \"32\"\n",
    "#     elif color == \"yellow\":\n",
    "#         col = \"33\"\n",
    "#     elif color == \"blue\":\n",
    "#         col = \"34\"\n",
    "#     elif color == \"purple\":\n",
    "#         col = \"35\"\n",
    "\n",
    "#     if style == \"bold\":\n",
    "#         sty = \"1\"\n",
    "#     elif style == \"italic\":\n",
    "#         sty = \"3\"\n",
    "#     elif style == \"curl\":\n",
    "#         sty = \"4\"\n",
    "#     elif style == \"blink1\":\n",
    "#         sty = \"5\"\n",
    "#     elif style == \"blink2\":\n",
    "#         sty = \"6\"\n",
    "#     elif style == \"selected\":\n",
    "#         sty = \"7\"\n",
    "\n",
    "#     return \"\\x1b[{}m\\x1b[{}m{}\\x1b[0m\".format(col,sty,string)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\"Prediction for: {}\".format(coloring(TARGET_COLUMN,\"yellow\")))\n",
    "\n",
    "# column_string = \"    {}    | {}  | {} |  {}\".format(coloring(\"Row\",\"blue\",\"bold\"),coloring(\"Expected\",\"green\",\"bold\"),coloring(\"Predicted\",\"purple\",\"bold\"),coloring(\"abs(Diff)\",\"red\",\"bold\"))\n",
    "# print(\"{:#<50s}\\n{}\\n{:-<50s}\".format(\"\",column_string,\"\"))\n",
    "# for i, row in enumerate(except_taget_rows.index.values):\n",
    "#     values_predict = de_scaled_prediction[i]\n",
    "#     values_is = except_taget_rows.loc[row][TARGET_COLUMN]\n",
    "#     # print(\"Zeile {} | {} (Predicted): {:.1f} , {} (Is): {:.1f}, |Diff|: {:.2f} \".format(row,TARGET_COLUMN,values_predict, TARGET_COLUMN ,values_is, abs(values_predict-values_is))) \n",
    "#     diff = abs(values_predict-values_is)\n",
    "#     print(\" {: ^22} | {: ^22} | {: ^22} | {: ^22}\".format(coloring(row,\"blue\"),coloring(f\"{values_predict:.1f}\",\"green\"),coloring(f\"{values_is:.1f}\",\"purple\"),coloring(f\"{diff:.1f}\",\"red\"))) \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ki_covid 3.10",
   "language": "python",
   "name": "ki_covid"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
